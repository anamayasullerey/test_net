# Introduction
nn_framework is designed for beginner level AI learners to experiment with neural network concepts. It is inspired by and covers the concepts covered in first three courses of the deep learning specialization by [Prof Andrew Ng](http://www.andrewng.org/ )  ([deeplearning.ai](https://www.deeplearning.ai/)).

# How to start
After downloading the git repository **"nnf/src"** directory needs to be added to **$PYTHONPATH**.

## 1. MNIST digit classifier example 
MNIST digit classfier example using nn_framework gives a feel of the architecture of nn_framework. It is recommended to follow the documentation and create a classifier in Jupyter notebook. The example, **mnist_net.py** is also checked in as python module and can be run in a shell.

## 2. nn_framework documentation
The documentation on nn_framework explains the featrues of the nn_framework. It also covers the important functions of the API.

# Modifying nn_framework
## Architecture of nn_framework
nn_framework builds a network as an array of layers. There layers have handles to their previous and next layers. Forward propagation and backward propagation is done as a recursion across the layers. Weight update functionality is separated from the layers. The state for weight update is maintained within the layers.

## Unit testing
nn_framework comes with unit tests. These tests are done on very small scale nets which can be debugged manually. It is **highly recommended** to add unit tests for any new features. The unit tests that come with the framework are divided into two categories
    1. **Gradient Tests** : These tests check the gradients of various combinations of layers and cost functs
    2. **Training Tests** : These tests train the network and check the preditions of the network. Training data for these tests is generated by a model function.
    
nn_framework comes with a 
# Modules
## neural_network
This is the class that captures overall architecture of the neural network. neural_network module is imported by the following statement.
```
import nn_framework.neural_network as nn

```
A neural network object is created by the following statement.
```
net = nn.NeuralNetwork("name", number_of_inputs)
```
Once created, layers are sequentially added to the network from input to output. The final layer is the output layer.
```
<create a layer>
net.add_layer(layer)
```
A L2 loss coefficient can be specified in the following manner.
```
net.set_l2_loss_coeff(l2_loss_coefficient)
```
A weight update method is added to the net.
```
net.set_weight_update_function(weight_update_parameters)
```
At this stage the network is defined. It can be initialized (random initialization) with the following code
```
net.initialize_parameters()
```
The following code trains the net.
```
# x : input 2D numpy array of size (number of inputs * batch size)
# y : output 2D numpy array of size (number of outputs * batch size)
net.train(x, y) 
```
The following code is used to predict the outputs.
```
y = net.predict(x) 
```
Other useful functions are,
```
loss = net.loss(y) # Returns loss. Called after net.forward_prop(x) or net.predict(x)
y = net.predict_classify(x) # Returns the index for classification based outputs
status = net.check_gradient(x, y) # Returns boolean. Numerically checks the gradient calculations.
net.print_state()
```
## layers
Layers in nn_framework is a entity that specifies forward propagation and backward propagation methods. Every layer stores activations and corresponding input derivatives (dactivations). The layers are stored in two layer dictionaries, one for hidden layers (hdict) and one for output layers (ldict). Layer dictionaries are imported by the following statement.
```
    import nn_framework.layer_dict as ld
```

### input layer
Inputl layer is a module of nn_framwork but it is automatically generated when a network instance is created. Users do not have to worry about this layer.

### hidden layers
Listed below are the input layer types and the code to generate them.
* fully connected (y = wx + b)
```
layer = ld.hdict["fc"](number_of_neurons)
```
* relu
```
layer = ld.hdict["relu"](number_of_neurons)
```
* sigmoid
```
layer = ld.hdict["sigmoid"](number_of_neurons)
```
* tanh
```
layer = ld.hdict["tanh"](number_of_neurons)
```

**_Note that "layer" frequently represents a fully connected function followed by an activation function. In nn_framework these are separate layers._**

### output layers
Listed below are the output layer types and the code to generate them.
* loss: This is the generic output layer that has an identity activation function (y=x). A loss function is specified the when this output layer is created. Following loss functions are supported for loss output layer.
o sigmoid_cross_entropy_loss
o linear_mean_squared_loss
```
layer = ld.odict["loss"]("linear_mean_squared_loss")
```
* sigmoid: This layer has a sigmoid activation function as well a sigmoid cross entropy loss.
```
layer = ld.odict["sigmoid"](number_of_neurons)
```
* softmax: This layer has a softmax activation function with a softmax cross entropy loss. When using this layer the outputs need to be logits (one hot binary set).
```
layer = ld.odict["softmax"](number_of_neurons)
```

